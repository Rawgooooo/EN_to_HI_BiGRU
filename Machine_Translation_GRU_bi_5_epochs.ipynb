{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kve7xov_nwlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Additional NOTE: THIS WAS A PAIN!!!!!!!!!!** _also fun_"
      ],
      "metadata": {
        "id": "RCQfZgg7nkR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-jUlxv49cOS",
        "outputId": "4e02d3d6-4ead-4f5c-af67-a53c9221c4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-29 11:57:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-29 11:57:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-29 11:57:34--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2025-05-29 12:00:14 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuU9fgwG28A-"
      },
      "source": [
        "#Tokenising English Sentences using GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5AZN8_6aWNsS"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(filepath, embedding_dim=100):\n",
        "    embeddings = {}\n",
        "    with open(filepath, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float)\n",
        "            if len(vector) == embedding_dim:\n",
        "                embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_path = \"glove.6B.100d.txt\" # We are using this glove embedding of dim =100\n",
        "glove = load_glove_embeddings(glove_path, embedding_dim=100)\n",
        "\n",
        "\n",
        "import string\n",
        "\n",
        "def simple_tokenize(sentence):\n",
        "    # Lowercase + split at whitespace\n",
        "    tokens = sentence.lower().strip().split()\n",
        "    # Remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # Filter out any empty tokens (e.g., \"\" from \"--\" or \"!!\")\n",
        "    return [w for w in stripped if w]\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def encode_glove(sentence, max_len=60, embedding_dim=100):\n",
        "    tokens = simple_tokenize(sentence.lower())[:max_len]\n",
        "    tensor = torch.zeros(max_len, embedding_dim)\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in glove:\n",
        "            tensor[i] = glove[token]\n",
        "        else:\n",
        "            tensor[i] = torch.zeros(embedding_dim)  # or random vector\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "6B5tZZXdXYNM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U8mMiAm_xPu",
        "outputId": "fd2bf754-b0b3-4aab-880a-13fcfe35f559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFcXfk6ma5mc"
      },
      "source": [
        "Reading the Translation data using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_csv(\"/content/translation_engtohin_train.csv\").dropna()\n",
        "source_sentences = df[\"source\"].tolist()\n",
        "target_sentences = df[\"target\"].tolist()\n",
        "\n",
        "# limiting source sentences because Colab keeps throwing Usage limit.\n",
        "# note: decreasing this did increase the errors to a good extent.\n",
        "num_sentences = 55000\n",
        "source_sentences = source_sentences[:num_sentences]\n",
        "target_sentences = target_sentences[:num_sentences]\n",
        "\n",
        "df = pd.read_csv(\"/content/translation_engtohin_valid.csv\").dropna()\n",
        "source_sentences_val = df[\"source\"].tolist()\n",
        "target_sentences_val = df[\"target\"].tolist()\n",
        "print(device)\n",
        "print(len(source_sentences))"
      ],
      "metadata": {
        "id": "tDWaH71YXfy_",
        "outputId": "fc1ff390-4a0e-44e3-b4c1-902605b974b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "55000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NtcBRn93J5H"
      },
      "source": [
        "#Tokenising Hindi Sentences using IndicBert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azxLIScPbKM7"
      },
      "source": [
        "*The below code can be used to analyse the length of sentences present in the dataset which is helpful to get an insight defining maximum length of source and target that you are gonna use to train the model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "92d35cb2bb7448a786cdf685d8d3a12b",
            "f31a0ed1b4a648e4be42c06db3a0fb47",
            "a21e2c5e61b24646966b2e26dfe0ba18",
            "db5b215d02974e4f99a0301c16df8db7",
            "53d03162461440b0b16e704fbb6cbaa3",
            "6dbb93dfba2c4bc785e20628aaedf1c3",
            "5e8a34cb80ed4762943a7f9062e4bd51",
            "5fc215e296b54ca9aafa19d4adb28906",
            "5f6ddc28449b4ad1bf9730a6542b340d",
            "b698854eeb9a48b58d74bae98e9c5f75",
            "d25e51a8fd414049b07e166350dc0a63",
            "8175b503fc484d99a96499625710eb69",
            "2f5810f079f945c99f5b38fc8b185dba",
            "99e8404b937e498ca430d9d437fd694d",
            "7e8943f6f0ea48489b562d6cd9494c6c",
            "8bfcbe39727f4ecda02af2c1c2a17d16",
            "8cd8a28ad4184af9bf55e4a60783ec24",
            "7f96be6bc4dc4c41b11fa8f2ed5a2b45",
            "e167afaa70fa45798627c9f59d836e88",
            "3009b497865f4f178cb82f30e174ce1d",
            "6512076f019f469d97981a68b06d6d3c",
            "c66c387c78fc43f9a179aa28d54469c9",
            "6aace65d2af540b88a52bb9b51019b14",
            "1733c6a25adc439eb6696a7b9e4cf217",
            "a70ff0cbefa34cd48cc9bb9c9c952f3a",
            "a4d863767cff469ab58c0c141ea25019",
            "d64b0c1703864d5eb1407e8a47e1852b",
            "703c314f4b9a4261b3db7357d91c6135",
            "e2e078cf00cb49fc9f944ca6cb5753c3",
            "d5fdb29d33c4405c80c0806cd1e9dc6c",
            "cdecf511ce57433683520340e079695e",
            "876ad96e521f47b5a967850042f30699",
            "721ac8b489654569833b02f4ba7e053b"
          ]
        },
        "id": "fgA-gWXS-3Y7",
        "outputId": "ce1fb5c6-316e-4523-daa8-1ec8197a33c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92d35cb2bb7448a786cdf685d8d3a12b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.75M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8175b503fc484d99a96499625710eb69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aace65d2af540b88a52bb9b51019b14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250000 hin_vocab_size\n",
            "Number of English sentences with >60 tokens: 457\n",
            "Number of Hindi sentences with >70 tokens: 1070\n"
          ]
        }
      ],
      "source": [
        "tokenizer_hin = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
        "hin_vocab_size = tokenizer_hin.vocab_size\n",
        "print(hin_vocab_size, \"hin_vocab_size\")\n",
        "\n",
        "import re\n",
        "\n",
        "def simple_tokenize_en(text):\n",
        "    return re.findall(r'\\w+', text)  # Split on words (alphanumeric)\n",
        "\n",
        "# Count for English\n",
        "long_en_count = sum(len(simple_tokenize_en(sent)) > 60 for sent in source_sentences)\n",
        "\n",
        "# Count for Marathi using IndicBERT tokenizer\n",
        "long_mr_count = sum(\n",
        "    len(tokenizer_hin(sent, truncation=False, padding=False)[\"input_ids\"]) > 70\n",
        "    for sent in target_sentences\n",
        ")\n",
        "\n",
        "print(f\"Number of English sentences with >60 tokens: {long_en_count}\")\n",
        "print(f\"Number of Hindi sentences with >70 tokens: {long_mr_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pF2CUQcgCAJ9"
      },
      "outputs": [],
      "source": [
        "def encode_hin(sentence, max_len=70):\n",
        "    token_ids = tokenizer_hin(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')[\"input_ids\"].squeeze(0)\n",
        "    return token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dvFr_J1vM5EL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, source_sentences, target_sentences, max_src_len, max_tgt_len):\n",
        "        self.source_sentences = source_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_vec = encode_glove(self.source_sentences[idx], self.max_src_len)  # [max_src_len, 100]\n",
        "        tgt_ids = encode_hin(self.target_sentences[idx], self.max_tgt_len)  # [max_tgt_len]\n",
        "        return src_vec, tgt_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    src_batch = torch.stack(src_batch)  # [B, max_src_len, 100]\n",
        "    tgt_batch = torch.stack(tgt_batch)  # [B, max_tgt_len]\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZ83F4N3smv"
      },
      "source": [
        "| Component               | Purpose                                               |\n",
        "| ----------------------- | ----------------------------------------------------- |\n",
        "| `simple_tokenize`       | Clean and tokenize English sentences manually         |\n",
        "| `load_glove_embeddings` | Load GloVe vectors into a dictionary                  |\n",
        "| `encode_glove`          | Convert English text to GloVe vector matrix           |\n",
        "| `AutoTokenizer`         | Pre-trained tokenizer for Hindi                       |\n",
        "| `encode_hin`            | Convert Hindi sentence into token IDs                 |\n",
        "| `TranslationDataset`    | Dataset to serve sentence pairs for training          |\n",
        "| `collate_fn`            | Create uniform batches from variable-length sentences |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LS4o0rgnK-ZZ"
      },
      "outputs": [],
      "source": [
        "max_src_len = 10 #decide yourself\n",
        "max_tgt_len = 10 #decide yourself\n",
        "batch_size = 64 #decide yourself (GPU ran out of memory for batch size of 128, and lowering to 32 or 16 SIGNIFICANTLY increased training time, it was pain)\n",
        "\n",
        "# Dataset and loaders\n",
        "train_dataset = TranslationDataset(source_sentences, target_sentences, max_src_len, max_tgt_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "val_dataset = TranslationDataset(source_sentences_val, target_sentences_val, max_src_len, max_tgt_len)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZca5qxQ7tvT",
        "outputId": "1103e682-fced-4b16-b7e9-d9531228650c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source dataset length English torch.Size([10, 100])\n",
            "Source dataset length Hindi torch.Size([10])\n",
            "torch.Size([10, 100])\n",
            "55000\n"
          ]
        }
      ],
      "source": [
        "print(\"Source dataset length English\",train_dataset[0][0].shape)\n",
        "print(\"Source dataset length Hindi\",train_dataset[0][1].shape)\n",
        "print(train_dataset[0][0].shape)\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK0TM4JiKwbb"
      },
      "source": [
        "*Your task is to build a sequence-to-sequence model using a RNN  to perform machine translation. The dataset is already preprocessed and tokenized, and embeddings for both source and target languages are provided. Implement the encoder-decoder architecture using RNNs, train your model on the training split, validate it on the validation split, and finally evaluate it on the test set using BLEU score.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sltxCVqqhNHv"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a Many-to-Many Architecture here\n",
        "with 3 hidden layers in encoder and decoder each."
      ],
      "metadata": {
        "id": "Nt5SHSWd4Ot7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qDDZHCazhNB1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "5b675a91-ce33-4dd1-efeb-674801d7f147"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nclass Encoder(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, num_layers):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.num_layers = num_layers\\n        # num_layers parameter specifies the number of recurrent layers\\n        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\\n\\n    def forward(self, src):\\n        # src: [batch_size, max_src_len, input_dim]\\n        # output: [batch_size, max_src_len, hidden_dim]\\n        # hidden: [num_layers, batch_size, hidden_dim]\\n        output, hidden = self.rnn(src)\\n        return hidden\\n\\nclass Decoder(nn.Module):\\n    def __init__(self, output_dim, hidden_dim, num_layers):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.output_dim = output_dim\\n        self.num_layers = num_layers\\n        # Embedding for target tokens, maps token IDs to hidden_dim vec\\n        self.embedding = nn.Embedding(output_dim, hidden_dim)\\n        self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, input, hidden):\\n        # input: [batch_size] (a batch of single token IDs at a time step)\\n        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\\n\\n        # Add a sequence dimension to the input for RNN\\n        input = input.unsqueeze(1) # [batch_size, 1]\\n\\n        # Embed the input token\\n        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\\n\\n        # Pass embedded input and previous hidden state through RNN\\n        # output: [batch_size, 1, hidden_dim]\\n        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state for all layers)\\n        output, hidden = self.rnn(embedded, hidden)\\n\\n        # Predict the next token (need to remove the sequence dimension for the linear layer)\\n        prediction = self.fc_out(output.squeeze(1)) # [batch_size, output_dim]\\n\\n        return prediction, hidden\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import pandas as pd\n",
        "'''\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        # num_layers parameter specifies the number of recurrent layers\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # output: [batch_size, max_src_len, hidden_dim]\n",
        "        # hidden: [num_layers, batch_size, hidden_dim]\n",
        "        output, hidden = self.rnn(src)\n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        # Embedding for target tokens, maps token IDs to hidden_dim vec\n",
        "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input: [batch_size] (a batch of single token IDs at a time step)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\n",
        "\n",
        "        # Add a sequence dimension to the input for RNN\n",
        "        input = input.unsqueeze(1) # [batch_size, 1]\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\n",
        "\n",
        "        # Pass embedded input and previous hidden state through RNN\n",
        "        # output: [batch_size, 1, hidden_dim]\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state for all layers)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "\n",
        "        # Predict the next token (need to remove the sequence dimension for the linear layer)\n",
        "        prediction = self.fc_out(output.squeeze(1)) # [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utmk1tcHuPzM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # trg: [batch_size, max_tgt_len] (contains token IDs)\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        max_tgt_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode source sentence\n",
        "        encoder_hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        # We need to assume a special token for the start of sequence.\n",
        "        # For IndicBERT, the <sos> token is the CLS token (usually 101)\n",
        "        # and the <eos> token is the SEP token (usually 102).\n",
        "        # We will use the CLS token as the initial input to the decoder.\n",
        "        input = trg[:, 0] # Use the first token of the target sequence (should be <sos>)\n",
        "\n",
        "        for t in range(1, max_tgt_len): # Start from 1 because the first token was the input\n",
        "            # Pass input token and hidden state to decoder\n",
        "            output, encoder_hidden = self.decoder(input, encoder_hidden)\n",
        "\n",
        "            # Store the output prediction\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Decide next input token\n",
        "            # Use teacher forcing (feed ground truth target token as next input)\n",
        "            input = trg[:, t]\n",
        "\n",
        "        return outputs\n",
        "'''\n",
        "'''\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        # Ensure encoder and decoder have the same number of layers for hidden state compatibility\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # trg: [batch_size, max_tgt_len] (contains token IDs)\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        max_tgt_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs (logits for each token at each time step)\n",
        "        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode source sentence to get the final hidden state for all layers\n",
        "        # encoder_hidden: [num_layers, batch_size, hidden_dim]\n",
        "        encoder_hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <sos> token (CLS token for IndicBERT)\n",
        "        input = trg[:, 0] # [batch_size]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        # Start from 1 because the first token (trg[:, 0]) was the initial input\n",
        "        for t in range(1, max_tgt_len):\n",
        "            # Pass input token and hidden state to decoder\n",
        "            # output: [batch_size, trg_vocab_size] (logits for next token)\n",
        "            # encoder_hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\n",
        "            output, encoder_hidden = self.decoder(input, encoder_hidden)\n",
        "\n",
        "            # Store the output prediction\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Decide next input token using teacher forcing (feed ground truth target token)\n",
        "            input = trg[:, t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# trial - Attention Module\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Linear layer to combine decoder hidden state and encoder outputs for 'importance' calculation\n",
        "        # hidden_dim * 2 because we concatenate decoder_hidden_last_layer and encoder_output\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        # Linear layer to project the importance to a scalar score\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_last_layer, encoder_outputs):\n",
        "        # decoder_hidden_last_layer: [batch_size, hidden_dim] (last layer hidden state of decoder)\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim] (all encoder outputs from encoder)\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        max_src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Repeat the decoder's last layer hidden state\n",
        "        # This prepares it for concatenation with each encoder output\n",
        "        # decoder_hidden_last_layer_repeated: [batch_size, max_src_len, hidden_dim]\n",
        "        decoder_hidden_last_layer_repeated = decoder_hidden_last_layer.unsqueeze(1).repeat(1, max_src_len, 1)\n",
        "\n",
        "        # Concatenate the repeated decoder hidden state with all encoder outputs\n",
        "        # energy_input: [batch_size, max_src_len, hidden_dim * 2]\n",
        "        energy_input = torch.cat((decoder_hidden_last_layer_repeated, encoder_outputs), dim=2) # energy is basically the importance\n",
        "\n",
        "        # Apply the attention scoring function (tanh and then linear projection)\n",
        "        # energy: [batch_size, max_src_len, hidden_dim]\n",
        "        energy = torch.tanh(self.attn(energy_input))\n",
        "\n",
        "        # Project the energy to a single scalar score for each source token\n",
        "        # attention_scores: [batch_size, max_src_len]\n",
        "        attention_scores = self.v(energy).squeeze(2)\n",
        "\n",
        "        # Apply softmax to get attention weights, ensuring they sum to 1 across the source sequence\n",
        "        # attention_weights: [batch_size, max_src_len]\n",
        "        return F.softmax(attention_scores, dim=1)\n",
        "\n",
        "'''\n",
        "# Encoder, Decoder, and Seq2Seq Classes with Attention\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        # num_layers parameter specifies the number of recurrent layers\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # output: [batch_size, max_src_len, hidden_dim] (all hidden states of the top layer)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (final hidden state for all layers)\n",
        "        output, hidden = self.rnn(src)\n",
        "        return output, hidden # Return both outputs and final hidden state\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, num_layers, attention):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention # Attention module instance\n",
        "\n",
        "        # Embedding for target language tokens, maps token IDs to hidden_dim vectors\n",
        "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        # RNN for the decoder\n",
        "        self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        # Final output layer: takes concatenated RNN output and context vector\n",
        "        # hidden_dim (from rnn_output) + hidden_dim (from context_vector)\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: [batch_size] (a batch of single token IDs at a time step)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim] (all encoder outputs)\n",
        "\n",
        "        # Add a sequence dimension to the input for RNN (RNN expects 3D input)\n",
        "        input = input.unsqueeze(1) # [batch_size, 1]\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\n",
        "\n",
        "        # Pass embedded input and previous hidden state through RNN\n",
        "        # rnn_output: [batch_size, 1, hidden_dim] (output of the top RNN layer for the current time step)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state for all layers)\n",
        "        rnn_output, hidden = self.rnn(embedded, hidden)\n",
        "\n",
        "        # Get the hidden state of the last layer of the decoder for attention calculation\n",
        "        # decoder_hidden_last_layer: [batch_size, hidden_dim]\n",
        "        decoder_hidden_last_layer = hidden[-1]\n",
        "\n",
        "        # Calculate attention weights based on decoder's current hidden state and all encoder outputs\n",
        "        # attention_weights: [batch_size, max_src_len]\n",
        "        attention_weights = self.attention(decoder_hidden_last_layer, encoder_outputs)\n",
        "\n",
        "        # Apply attention weights to encoder outputs to get the context vector\n",
        "        # context_vector: [batch_size, hidden_dim]\n",
        "        # torch.bmm performs batch matrix-matrix product: (B, 1, L) x (B, L, H) -> (B, 1, H)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        # Concatenate the decoder RNN output (squeezed to remove sequence dim) with the context vector\n",
        "        # combined_input_for_fc: [batch_size, hidden_dim * 2]\n",
        "        combined_input_for_fc = torch.cat((rnn_output.squeeze(1), context_vector), dim=1)\n",
        "\n",
        "        # Predict the next token using the combined information\n",
        "        prediction = self.fc_out(combined_input_for_fc) # [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        # Ensure encoder and decoder have the same number of layers for hidden state compatibility\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # trg: [batch_size, max_tgt_len] (contains token IDs)\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        max_tgt_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs (logits for each token at each time step)\n",
        "        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode source sentence to get all hidden states (for attention)\n",
        "        # and the final hidden state (to initialize decoder)\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim]\n",
        "        # encoder_hidden: [num_layers, batch_size, hidden_dim]\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <sos> token (CLS token for IndicBERT)\n",
        "        input = trg[:, 0] # [batch_size]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        # Start from 1 because the first token (trg[:, 0]) was the initial input\n",
        "        for t in range(1, max_tgt_len):\n",
        "            # Pass input token, hidden state, and all encoder outputs to decoder\n",
        "            # output: [batch_size, trg_vocab_size] (logits for next token)\n",
        "            # encoder_hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\n",
        "            output, encoder_hidden = self.decoder(input, encoder_hidden, encoder_outputs)\n",
        "\n",
        "            # Store the output prediction\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Decide next input token using teacher forcing (feed ground truth target token)\n",
        "            input = trg[:, t]\n",
        "\n",
        "        return outputs\n",
        "'''\n",
        "\n",
        "\n",
        "## Encoder using GRU\n",
        "'''\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        # Use nn.GRU instead of nn.RNN or nn.LSTM\n",
        "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # output: [batch_size, max_src_len, hidden_dim] (all hidden states of the top layer)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (final hidden state for all layers)\n",
        "        output, hidden = self.rnn(src)\n",
        "        # Return all outputs (for attention) and the final hidden state\n",
        "        return output, hidden\n",
        "'''\n",
        "\n",
        "## Encoder using Bidirectional GRU (Refined)\n",
        "\n",
        "class EncoderBiGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        # Use nn.GRU with bidirectional=True\n",
        "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # We will need a way to initialize the decoder's hidden state\n",
        "        # which is typically of size [num_layers, batch_size, hidden_dim].\n",
        "        # The bidirectional encoder's final hidden state is [num_layers * 2, batch_size, hidden_dim].\n",
        "        # A common approach is to concatenate the forward and backward final states\n",
        "        # and pass them through a linear layer to match the decoder's hidden size.\n",
        "        # Here, we will concatenate the final hidden states for each layer.\n",
        "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        # Initialize this linear layer with appropriate weights (optional but can help)\n",
        "        # nn.init.xavier_uniform_(self.fc_hidden.weight) # Example initialization\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # output: [batch_size, max_src_len, hidden_dim * 2]\n",
        "        # hidden: [num_layers * 2, batch_size, hidden_dim]\n",
        "        output, hidden = self.rnn(src)\n",
        "\n",
        "        # Combine forward and backward final hidden states for each layer\n",
        "        # hidden_forward: [num_layers, batch_size, hidden_dim]\n",
        "        # hidden_backward: [num_layers, batch_size, hidden_dim]\n",
        "        hidden_forward = hidden[0:self.num_layers]\n",
        "        hidden_backward = hidden[self.num_layers:]\n",
        "\n",
        "        # Concatenate forward and backward states for each layer\n",
        "        # combined_hidden: [num_layers, batch_size, hidden_dim * 2]\n",
        "        combined_hidden = torch.cat((hidden_forward, hidden_backward), dim=2)\n",
        "\n",
        "        # Project the combined hidden state to the decoder's expected hidden dimension\n",
        "        # final_encoder_hidden: [num_layers, batch_size, hidden_dim]\n",
        "        final_encoder_hidden = torch.relu(self.fc_hidden(combined_hidden)) # Use ReLU or Tanh activation\n",
        "\n",
        "        # Return all time-step outputs (for attention) and the processed final hidden state (for decoder initialization)\n",
        "        return output, final_encoder_hidden\n",
        "\n",
        "\n",
        "## Decoder using GRU with Attention\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, num_layers, attention):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention # Attention module instance\n",
        "\n",
        "        # Embedding for target language tokens\n",
        "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        # Use nn.GRU instead of nn.RNN or nn.LSTM\n",
        "        # Input dimension is hidden_dim (from embedding) + hidden_dim (from context vector)\n",
        "        self.rnn = nn.GRU(hidden_dim * 2, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        # Final output layer: takes concatenated RNN output and context vector\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: [batch_size] (a batch of single token IDs at a time step)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim] (all encoder outputs)\n",
        "\n",
        "        # Add a sequence dimension to the input for GRU\n",
        "        input = input.unsqueeze(1) # [batch_size, 1]\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\n",
        "\n",
        "        # Get the hidden state of the last layer of the decoder for attention calculation\n",
        "        # decoder_hidden_last_layer: [batch_size, hidden_dim]\n",
        "        decoder_hidden_last_layer = hidden[-1]\n",
        "\n",
        "        # Calculate attention weights based on decoder's current hidden state and all encoder outputs\n",
        "        # attention_weights: [batch_size, max_src_len]\n",
        "        attention_weights = self.attention(decoder_hidden_last_layer, encoder_outputs)\n",
        "\n",
        "        # Apply attention weights to encoder outputs to get the context vector\n",
        "        # context_vector: [batch_size, hidden_dim]\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        # Concatenate the embedded input with the context vector for the GRU input\n",
        "        # embedded_with_context: [batch_size, 1, hidden_dim * 2]\n",
        "        embedded_with_context = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)\n",
        "\n",
        "        # Pass combined input and previous hidden state through GRU\n",
        "        # rnn_output: [batch_size, 1, hidden_dim] (output of the top GRU layer)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\n",
        "        rnn_output, hidden = self.rnn(embedded_with_context, hidden)\n",
        "\n",
        "        # Predict the next token using the RNN output and the context vector\n",
        "        # Combine rnn_output (squeezed) and context_vector (already squeezed)\n",
        "        combined_input_for_fc = torch.cat((rnn_output.squeeze(1), context_vector), dim=1)\n",
        "\n",
        "        prediction = self.fc_out(combined_input_for_fc) # [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden # Return prediction and updated hidden state\n",
        "\n",
        "## Seq2Seq Model with GRU and Attention\n",
        "\n",
        "class Seq2SeqGRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        # Ensure encoder and decoder have the same number of layers\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # trg: [batch_size, max_tgt_len] (contains token IDs)\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        max_tgt_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs (logits)\n",
        "        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode source sentence\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim]\n",
        "        # encoder_hidden: [num_layers, batch_size, hidden_dim]\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
        "\n",
        "        # Initialize decoder's hidden state from the encoder's final hidden state\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        input = trg[:, 0] # [batch_size]\n",
        "\n",
        "        # Iterate through the target sequence (excluding the <sos> token at index 0)\n",
        "        for t in range(1, max_tgt_len):\n",
        "            # Pass input token, hidden state, and encoder outputs to decoder\n",
        "            # output: [batch_size, trg_vocab_size] (logits for next token)\n",
        "            # decoder_hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\n",
        "            output, decoder_hidden = self.decoder(\n",
        "                input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Store the output prediction\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Decide next input token using teacher forcing\n",
        "            input = trg[:, t]\n",
        "\n",
        "        return outputs\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A2p06GlKH7m2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "dd53091b-9eec-44eb-f72d-875b46517272"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# trial - Attention Module \\nclass Attention(nn.Module):\\n    def __init__(self, hidden_dim):\\n        super().__init__()\\n        # Linear layer to combine decoder hidden state and encoder outputs for \\'importance\\' calculation\\n        # hidden_dim * 2 because we concatenate decoder_hidden_last_layer and encoder_output\\n        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\\n        # Linear layer to project the importance to a scalar score\\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\\n\\n    def forward(self, decoder_hidden_last_layer, encoder_outputs):\\n        # decoder_hidden_last_layer: [batch_size, hidden_dim] (last layer hidden state of decoder)\\n        # encoder_outputs: [batch_size, max_src_len, hidden_dim] (all encoder outputs from encoder)\\n\\n        batch_size = encoder_outputs.shape[0]\\n        max_src_len = encoder_outputs.shape[1]\\n\\n        # Repeat the decoder\\'s last layer hidden state \\n        # This prepares it for concatenation with each encoder output\\n        # decoder_hidden_last_layer_repeated: [batch_size, max_src_len, hidden_dim]\\n        decoder_hidden_last_layer_repeated = decoder_hidden_last_layer.unsqueeze(1).repeat(1, max_src_len, 1)\\n\\n        # Concatenate the repeated decoder hidden state with all encoder outputs\\n        # energy_input: [batch_size, max_src_len, hidden_dim * 2]\\n        energy_input = torch.cat((decoder_hidden_last_layer_repeated, encoder_outputs), dim=2) # energy is basically the importance\\n\\n        # Apply the attention scoring function (tanh and then linear projection)\\n        # energy: [batch_size, max_src_len, hidden_dim]\\n        energy = torch.tanh(self.attn(energy_input))\\n\\n        # Project the energy to a single scalar score for each source token\\n        # attention_scores: [batch_size, max_src_len]\\n        attention_scores = self.v(energy).squeeze(2)\\n\\n        # Apply softmax to get attention weights, ensuring they sum to 1 across the source sequence\\n        # attention_weights: [batch_size, max_src_len]\\n        return F.softmax(attention_scores, dim=1)\\n\\n\\'\\'\\'\\n# Encoder, Decoder, and Seq2Seq Classes with Attention\\nclass Encoder(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, num_layers):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.num_layers = num_layers\\n        # num_layers parameter specifies the number of recurrent layers\\n        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\\n\\n    def forward(self, src):\\n        # src: [batch_size, max_src_len, input_dim]\\n        # output: [batch_size, max_src_len, hidden_dim] (all hidden states of the top layer)\\n        # hidden: [num_layers, batch_size, hidden_dim] (final hidden state for all layers)\\n        output, hidden = self.rnn(src)\\n        return output, hidden # Return both outputs and final hidden state\\n\\n\\nclass Decoder(nn.Module):\\n    def __init__(self, output_dim, hidden_dim, num_layers, attention):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.output_dim = output_dim\\n        self.num_layers = num_layers\\n        self.attention = attention # Attention module instance\\n\\n        # Embedding for target language tokens, maps token IDs to hidden_dim vectors\\n        self.embedding = nn.Embedding(output_dim, hidden_dim)\\n        # RNN for the decoder\\n        self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\\n        # Final output layer: takes concatenated RNN output and context vector\\n        # hidden_dim (from rnn_output) + hidden_dim (from context_vector)\\n        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\\n\\n    def forward(self, input, hidden, encoder_outputs):\\n        # input: [batch_size] (a batch of single token IDs at a time step)\\n        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\\n        # encoder_outputs: [batch_size, max_src_len, hidden_dim] (all encoder outputs)\\n\\n        # Add a sequence dimension to the input for RNN (RNN expects 3D input)\\n        input = input.unsqueeze(1) # [batch_size, 1]\\n\\n        # Embed the input token\\n        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\\n\\n        # Pass embedded input and previous hidden state through RNN\\n        # rnn_output: [batch_size, 1, hidden_dim] (output of the top RNN layer for the current time step)\\n        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state for all layers)\\n        rnn_output, hidden = self.rnn(embedded, hidden)\\n\\n        # Get the hidden state of the last layer of the decoder for attention calculation\\n        # decoder_hidden_last_layer: [batch_size, hidden_dim]\\n        decoder_hidden_last_layer = hidden[-1]\\n\\n        # Calculate attention weights based on decoder\\'s current hidden state and all encoder outputs\\n        # attention_weights: [batch_size, max_src_len]\\n        attention_weights = self.attention(decoder_hidden_last_layer, encoder_outputs)\\n\\n        # Apply attention weights to encoder outputs to get the context vector\\n        # context_vector: [batch_size, hidden_dim]\\n        # torch.bmm performs batch matrix-matrix product: (B, 1, L) x (B, L, H) -> (B, 1, H)\\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\\n\\n        # Concatenate the decoder RNN output (squeezed to remove sequence dim) with the context vector\\n        # combined_input_for_fc: [batch_size, hidden_dim * 2]\\n        combined_input_for_fc = torch.cat((rnn_output.squeeze(1), context_vector), dim=1)\\n\\n        # Predict the next token using the combined information\\n        prediction = self.fc_out(combined_input_for_fc) # [batch_size, output_dim]\\n\\n        return prediction, hidden\\n\\nclass Seq2Seq(nn.Module):\\n    def __init__(self, encoder, decoder, device):\\n        super().__init__()\\n        self.encoder = encoder\\n        self.decoder = decoder\\n        self.device = device\\n\\n        # Ensure encoder and decoder have the same number of layers for hidden state compatibility\\n        assert encoder.num_layers == decoder.num_layers,             \"Encoder and decoder must have equal number of layers!\"\\n\\n    def forward(self, src, trg):\\n        # src: [batch_size, max_src_len, input_dim]\\n        # trg: [batch_size, max_tgt_len] (contains token IDs)\\n\\n        batch_size = trg.shape[0]\\n        max_tgt_len = trg.shape[1]\\n        trg_vocab_size = self.decoder.output_dim\\n\\n        # Tensor to store decoder outputs (logits for each token at each time step)\\n        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\\n\\n        # Encode source sentence to get all hidden states (for attention)\\n        # and the final hidden state (to initialize decoder)\\n        # encoder_outputs: [batch_size, max_src_len, hidden_dim]\\n        # encoder_hidden: [num_layers, batch_size, hidden_dim]\\n        encoder_outputs, encoder_hidden = self.encoder(src)\\n\\n        # First input to the decoder is the <sos> token (CLS token for IndicBERT)\\n        input = trg[:, 0] # [batch_size]\\n\\n        # Iterate through the target sequence, predicting one token at a time\\n        # Start from 1 because the first token (trg[:, 0]) was the initial input\\n        for t in range(1, max_tgt_len):\\n            # Pass input token, hidden state, and all encoder outputs to decoder\\n            # output: [batch_size, trg_vocab_size] (logits for next token)\\n            # encoder_hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\\n            output, encoder_hidden = self.decoder(input, encoder_hidden, encoder_outputs)\\n\\n            # Store the output prediction\\n            outputs[:, t, :] = output\\n\\n            # Decide next input token using teacher forcing (feed ground truth target token)\\n            input = trg[:, t]\\n\\n        return outputs\\n\\'\\'\\'\\n\\n\\n## Encoder using GRU\\n\\'\\'\\'\\nclass EncoderGRU(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, num_layers):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.num_layers = num_layers\\n        # Use nn.GRU instead of nn.RNN or nn.LSTM\\n        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\\n\\n    def forward(self, src):\\n        # src: [batch_size, max_src_len, input_dim]\\n        # output: [batch_size, max_src_len, hidden_dim] (all hidden states of the top layer)\\n        # hidden: [num_layers, batch_size, hidden_dim] (final hidden state for all layers)\\n        output, hidden = self.rnn(src)\\n        # Return all outputs (for attention) and the final hidden state\\n        return output, hidden\\n\\'\\'\\'\\n\\n## Encoder using Bidirectional GRU (Refined)\\n\\nclass EncoderBiGRU(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, num_layers):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.num_layers = num_layers\\n        # Use nn.GRU with bidirectional=True\\n        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\\n\\n        # We will need a way to initialize the decoder\\'s hidden state\\n        # which is typically of size [num_layers, batch_size, hidden_dim].\\n        # The bidirectional encoder\\'s final hidden state is [num_layers * 2, batch_size, hidden_dim].\\n        # A common approach is to concatenate the forward and backward final states\\n        # and pass them through a linear layer to match the decoder\\'s hidden size.\\n        # Here, we will concatenate the final hidden states for each layer.\\n        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\\n        # Initialize this linear layer with appropriate weights (optional but can help)\\n        # nn.init.xavier_uniform_(self.fc_hidden.weight) # Example initialization\\n\\n    def forward(self, src):\\n        # src: [batch_size, max_src_len, input_dim]\\n        # output: [batch_size, max_src_len, hidden_dim * 2]\\n        # hidden: [num_layers * 2, batch_size, hidden_dim]\\n        output, hidden = self.rnn(src)\\n\\n        # Combine forward and backward final hidden states for each layer\\n        # hidden_forward: [num_layers, batch_size, hidden_dim]\\n        # hidden_backward: [num_layers, batch_size, hidden_dim]\\n        hidden_forward = hidden[0:self.num_layers]\\n        hidden_backward = hidden[self.num_layers:]\\n\\n        # Concatenate forward and backward states for each layer\\n        # combined_hidden: [num_layers, batch_size, hidden_dim * 2]\\n        combined_hidden = torch.cat((hidden_forward, hidden_backward), dim=2)\\n\\n        # Project the combined hidden state to the decoder\\'s expected hidden dimension\\n        # final_encoder_hidden: [num_layers, batch_size, hidden_dim]\\n        final_encoder_hidden = torch.relu(self.fc_hidden(combined_hidden)) # Use ReLU or Tanh activation\\n\\n        # Return all time-step outputs (for attention) and the processed final hidden state (for decoder initialization)\\n        return output, final_encoder_hidden\\n\\n\\n## Decoder using GRU with Attention\\n\\nclass DecoderGRU(nn.Module):\\n    def __init__(self, output_dim, hidden_dim, num_layers, attention):\\n        super().__init__()\\n        self.hidden_dim = hidden_dim\\n        self.output_dim = output_dim\\n        self.num_layers = num_layers\\n        self.attention = attention # Attention module instance\\n\\n        # Embedding for target language tokens\\n        self.embedding = nn.Embedding(output_dim, hidden_dim)\\n        # Use nn.GRU instead of nn.RNN or nn.LSTM\\n        # Input dimension is hidden_dim (from embedding) + hidden_dim (from context vector)\\n        self.rnn = nn.GRU(hidden_dim * 2, hidden_dim, num_layers=num_layers, batch_first=True)\\n        # Final output layer: takes concatenated RNN output and context vector\\n        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\\n\\n    def forward(self, input, hidden, encoder_outputs):\\n        # input: [batch_size] (a batch of single token IDs at a time step)\\n        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\\n        # encoder_outputs: [batch_size, max_src_len, hidden_dim] (all encoder outputs)\\n\\n        # Add a sequence dimension to the input for GRU\\n        input = input.unsqueeze(1) # [batch_size, 1]\\n\\n        # Embed the input token\\n        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\\n\\n        # Get the hidden state of the last layer of the decoder for attention calculation\\n        # decoder_hidden_last_layer: [batch_size, hidden_dim]\\n        decoder_hidden_last_layer = hidden[-1]\\n\\n        # Calculate attention weights based on decoder\\'s current hidden state and all encoder outputs\\n        # attention_weights: [batch_size, max_src_len]\\n        attention_weights = self.attention(decoder_hidden_last_layer, encoder_outputs)\\n\\n        # Apply attention weights to encoder outputs to get the context vector\\n        # context_vector: [batch_size, hidden_dim]\\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\\n\\n        # Concatenate the embedded input with the context vector for the GRU input\\n        # embedded_with_context: [batch_size, 1, hidden_dim * 2]\\n        embedded_with_context = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)\\n\\n        # Pass combined input and previous hidden state through GRU\\n        # rnn_output: [batch_size, 1, hidden_dim] (output of the top GRU layer)\\n        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\\n        rnn_output, hidden = self.rnn(embedded_with_context, hidden)\\n\\n        # Predict the next token using the RNN output and the context vector\\n        # Combine rnn_output (squeezed) and context_vector (already squeezed)\\n        combined_input_for_fc = torch.cat((rnn_output.squeeze(1), context_vector), dim=1)\\n\\n        prediction = self.fc_out(combined_input_for_fc) # [batch_size, output_dim]\\n\\n        return prediction, hidden # Return prediction and updated hidden state\\n\\n## Seq2Seq Model with GRU and Attention\\n\\nclass Seq2SeqGRU(nn.Module):\\n    def __init__(self, encoder, decoder, device):\\n        super().__init__()\\n        self.encoder = encoder\\n        self.decoder = decoder\\n        self.device = device\\n\\n        # Ensure encoder and decoder have the same number of layers\\n        assert encoder.num_layers == decoder.num_layers,             \"Encoder and decoder must have equal number of layers!\"\\n\\n    def forward(self, src, trg):\\n        # src: [batch_size, max_src_len, input_dim]\\n        # trg: [batch_size, max_tgt_len] (contains token IDs)\\n\\n        batch_size = trg.shape[0]\\n        max_tgt_len = trg.shape[1]\\n        trg_vocab_size = self.decoder.output_dim\\n\\n        # Tensor to store decoder outputs (logits)\\n        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\\n\\n        # Encode source sentence\\n        # encoder_outputs: [batch_size, max_src_len, hidden_dim]\\n        # encoder_hidden: [num_layers, batch_size, hidden_dim]\\n        encoder_outputs, encoder_hidden = self.encoder(src)\\n\\n        # Initialize decoder\\'s hidden state from the encoder\\'s final hidden state\\n        decoder_hidden = encoder_hidden\\n\\n        # First input to the decoder is the <sos> token\\n        input = trg[:, 0] # [batch_size]\\n\\n        # Iterate through the target sequence (excluding the <sos> token at index 0)\\n        for t in range(1, max_tgt_len):\\n            # Pass input token, hidden state, and encoder outputs to decoder\\n            # output: [batch_size, trg_vocab_size] (logits for next token)\\n            # decoder_hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\\n            output, decoder_hidden = self.decoder(\\n                input, decoder_hidden, encoder_outputs\\n            )\\n\\n            # Store the output prediction\\n            outputs[:, t, :] = output\\n\\n            # Decide next input token using teacher forcing\\n            input = trg[:, t]\\n\\n        return outputs\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trial - Attention Module\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Linear layer to combine decoder hidden state and encoder outputs for 'importance' calculation\n",
        "        # hidden_dim (decoder last layer) + hidden_dim * 2 (bidirectional encoder output)\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)  # Corrected input dimension\n",
        "        # Linear layer to project the importance to a scalar score\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_last_layer, encoder_outputs):\n",
        "        # decoder_hidden_last_layer: [batch_size, hidden_dim] (last layer hidden state of decoder)\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim * 2] (all encoder outputs from *bidirectional* encoder)\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        max_src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Repeat the decoder's last layer hidden state\n",
        "        # This prepares it for concatenation with each encoder output\n",
        "        # decoder_hidden_last_layer_repeated: [batch_size, max_src_len, hidden_dim]\n",
        "        decoder_hidden_last_layer_repeated = decoder_hidden_last_layer.unsqueeze(1).repeat(1, max_src_len, 1)\n",
        "\n",
        "        # Concatenate the repeated decoder hidden state with all encoder outputs\n",
        "        # energy_input: [batch_size, max_src_len, hidden_dim + hidden_dim * 2] = [batch_size, max_src_len, hidden_dim * 3]\n",
        "        energy_input = torch.cat((decoder_hidden_last_layer_repeated, encoder_outputs), dim=2)\n",
        "\n",
        "        # Apply the attention scoring function (tanh and then linear projection)\n",
        "        # energy: [batch_size, max_src_len, hidden_dim]\n",
        "        energy = torch.tanh(self.attn(energy_input))\n",
        "\n",
        "        # Project the energy to a single scalar score for each source token\n",
        "        # attention_scores: [batch_size, max_src_len]\n",
        "        attention_scores = self.v(energy).squeeze(2)\n",
        "\n",
        "        # Apply softmax to get attention weights, ensuring they sum to 1 across the source sequence\n",
        "        # attention_weights: [batch_size, max_src_len]\n",
        "        return F.softmax(attention_scores, dim=1)\n",
        "\n",
        "\n",
        "## Encoder using Bidirectional GRU (Refined)\n",
        "\n",
        "class EncoderBiGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        # Use nn.GRU with bidirectional=True\n",
        "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Linear layer to project the concatenated forward/backward final hidden states\n",
        "        # from hidden_dim * 2 to hidden_dim to match the decoder's hidden state size.\n",
        "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        # Initialize this linear layer with appropriate weights (optional but can help)\n",
        "        # nn.init.xavier_uniform_(self.fc_hidden.weight) # Example initialization\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # output: [batch_size, max_src_len, hidden_dim * 2]\n",
        "        # hidden: [num_layers * 2, batch_size, hidden_dim]\n",
        "        output, hidden = self.rnn(src)\n",
        "\n",
        "        # Combine forward and backward final hidden states for each layer\n",
        "        # hidden_forward: [num_layers, batch_size, hidden_dim]\n",
        "        # hidden_backward: [num_layers, batch_size, hidden_dim]\n",
        "        hidden_forward = hidden[0:self.num_layers]\n",
        "        hidden_backward = hidden[self.num_layers:]\n",
        "\n",
        "        # Concatenate forward and backward states for each layer\n",
        "        # combined_hidden: [num_layers, batch_size, hidden_dim * 2]\n",
        "        combined_hidden = torch.cat((hidden_forward, hidden_backward), dim=2)\n",
        "\n",
        "        # Project the combined hidden state to the decoder's expected hidden dimension\n",
        "        # final_encoder_hidden: [num_layers, batch_size, hidden_dim]\n",
        "        final_encoder_hidden = torch.relu(self.fc_hidden(combined_hidden)) # Use ReLU or Tanh activation\n",
        "\n",
        "        # Return all time-step outputs (for attention) and the processed final hidden state (for decoder initialization)\n",
        "        return output, final_encoder_hidden\n",
        "\n",
        "## Decoder using GRU with Attention\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, num_layers, attention):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention # Attention module instance\n",
        "\n",
        "        # Embedding for target language tokens\n",
        "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        # Use nn.GRU instead of nn.RNN or nn.LSTM\n",
        "        # Input dimension is hidden_dim (from embedding) + hidden_dim*2 (from context vector)\n",
        "        self.rnn = nn.GRU(hidden_dim * 3, hidden_dim, num_layers=num_layers, batch_first=True) # <--- Change this line\n",
        "        # Final output layer: takes concatenated RNN output (hidden_dim) and context vector (hidden_dim*2)\n",
        "        self.fc_out = nn.Linear(hidden_dim * 3, output_dim) # <--- Change this line\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: [batch_size] (a batch of single token IDs at a time step)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (initial hidden state from encoder or previous decoder step)\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim * 2] (all encoder outputs from *bidirectional* encoder)\n",
        "\n",
        "        # Add a sequence dimension to the input for GRU\n",
        "        input = input.unsqueeze(1) # [batch_size, 1]\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input) # [batch_size, 1, hidden_dim]\n",
        "\n",
        "        # Get the hidden state of the last layer of the decoder for attention calculation\n",
        "        # decoder_hidden_last_layer: [batch_size, hidden_dim]\n",
        "        decoder_hidden_last_layer = hidden[-1]\n",
        "\n",
        "        # Calculate attention weights based on decoder's current hidden state and all encoder outputs\n",
        "        # attention_weights: [batch_size, max_src_len]\n",
        "        attention_weights = self.attention(decoder_hidden_last_layer, encoder_outputs)\n",
        "\n",
        "        # Apply attention weights to encoder outputs to get the context vector\n",
        "        # context_vector: [batch_size, hidden_dim * 2]\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        # Concatenate the embedded input with the context vector for the GRU input\n",
        "        # embedded_with_context: [batch_size, 1, hidden_dim + hidden_dim * 2] = [batch_size, 1, hidden_dim * 3]\n",
        "        embedded_with_context = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)\n",
        "\n",
        "        # Pass combined input and previous hidden state through GRU\n",
        "        # rnn_output: [batch_size, 1, hidden_dim] (output of the top GRU layer)\n",
        "        # hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\n",
        "        rnn_output, hidden = self.rnn(embedded_with_context, hidden)\n",
        "\n",
        "        # Predict the next token using the RNN output and the context vector\n",
        "        # Combine rnn_output (squeezed, hidden_dim) and context_vector (already squeezed, hidden_dim * 2)\n",
        "        # combined_input_for_fc: [batch_size, hidden_dim + hidden_dim * 2] = [batch_size, hidden_dim * 3]\n",
        "        combined_input_for_fc = torch.cat((rnn_output.squeeze(1), context_vector), dim=1)\n",
        "\n",
        "        prediction = self.fc_out(combined_input_for_fc) # [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden # Return prediction and updated hidden state\n",
        "\n",
        "\n",
        "## Seq2Seq Model with GRU and Attention\n",
        "\n",
        "class Seq2SeqGRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        # Ensure encoder and decoder have the same number of layers\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, max_src_len, input_dim]\n",
        "        # trg: [batch_size, max_tgt_len] (contains token IDs)\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        max_tgt_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs (logits)\n",
        "        outputs = torch.zeros(batch_size, max_tgt_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode source sentence\n",
        "        # encoder_outputs: [batch_size, max_src_len, hidden_dim]\n",
        "        # encoder_hidden: [num_layers, batch_size, hidden_dim]\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
        "\n",
        "        # Initialize decoder's hidden state from the encoder's final hidden state\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        input = trg[:, 0] # [batch_size]\n",
        "\n",
        "        # Iterate through the target sequence (excluding the <sos> token at index 0)\n",
        "        for t in range(1, max_tgt_len):\n",
        "            # Pass input token, hidden state, and encoder outputs to decoder\n",
        "            # output: [batch_size, trg_vocab_size] (logits for next token)\n",
        "            # decoder_hidden: [num_layers, batch_size, hidden_dim] (updated hidden state)\n",
        "            output, decoder_hidden = self.decoder(\n",
        "                input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Store the output prediction\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Decide next input token using teacher forcing\n",
        "            input = trg[:, t]\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "MoTk2RIo7mS0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "rrYOFrdPhNEM",
        "outputId": "1ceb63ae-e4ff-41a5-d4cf-8423f6ef288c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/1719\n",
            "Batch 500/1719\n",
            "Batch 1000/1719\n",
            "Batch 1500/1719\n",
            "Epoch: 01 | Train Loss: 6.038\n",
            "Epoch: 01 | Val Loss: 5.225\n",
            "Batch 0/1719\n",
            "Batch 500/1719\n",
            "Batch 1000/1719\n",
            "Batch 1500/1719\n",
            "Epoch: 02 | Train Loss: 4.546\n",
            "Epoch: 02 | Val Loss: 4.799\n",
            "Batch 0/1719\n",
            "Batch 500/1719\n",
            "Batch 1000/1719\n",
            "Batch 1500/1719\n",
            "Epoch: 03 | Train Loss: 3.512\n",
            "Epoch: 03 | Val Loss: 4.767\n",
            "Batch 0/1719\n",
            "Batch 500/1719\n",
            "Batch 1000/1719\n",
            "Batch 1500/1719\n",
            "Epoch: 04 | Train Loss: 2.604\n",
            "Epoch: 04 | Val Loss: 4.954\n",
            "Batch 0/1719\n",
            "Batch 500/1719\n",
            "Batch 1000/1719\n",
            "Batch 1500/1719\n",
            "Epoch: 05 | Train Loss: 2.011\n",
            "Epoch: 05 | Val Loss: 5.175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef calculate_bleu(data_loader, model, max_src_len, max_tgt_len, device, tokenizer_hin):\\n    trgs = []\\n    pred_trgs = []\\n\\n    model.eval()\\n\\n    with torch.no_grad():\\n        for i, (src, trg) in enumerate(data_loader):\\n            # We need the original target sentences as strings for BLEU, not the padded tensors\\n            # The DataLoader provides the padded tensors, so we need to get the original\\n            # sentences corresponding to this batch from the validation dataset.\\n            # This is not ideal for large datasets, but for demonstration, we can\\n            # retrieve the original sentences. A better approach would be to store\\n            # original sentences in the dataset or pass them alongside tensors.\\n\\n            # For this example, we\\'ll translate sentences one by one from the validation dataset\\n            # based on the batch index, which is inefficient but works for demonstration.\\n            # In a real scenario, process batches more efficiently.\\n\\n            batch_original_src = [source_sentences_val[j + i * data_loader.batch_size] for j in range(src.shape[0])]\\n            batch_original_trg = [target_sentences_val[j + i * data_loader.batch_size] for j in range(trg.shape[0])]\\n\\n\\n            for j in range(len(batch_original_src)):\\n                src_sentence = batch_original_src[j]\\n                trg_sentence = batch_original_trg[j]\\n\\n                # Tokenize reference sentence for BLEU\\n                # IndicBERT tokenizer might produce sub-words, BLEU works best on word tokens\\n                # Let\\'s try tokenizing the reference using a simple split or a more robust tokenizer if needed\\n                # For simplicity, we will use the IndicBERT tokenizer output for both prediction and reference\\n                # This might slightly lower the BLEU score compared to using word tokens for reference.\\n                # A more standard approach is to tokenize reference by spaces.\\n                reference_tokens = tokenizer_hin(trg_sentence, truncation=False, padding=False)[\"input_ids\"]\\n                # Remove special tokens from reference for BLEU calculation\\n                reference_tokens_filtered = [tok for tok in reference_tokens if tok not in tokenizer_hin.all_special_ids]\\n                reference = [tokenizer_hin.decode(reference_tokens_filtered).split()] # BLEU expects list of reference translations\\n\\n                # Translate the source sentence\\n                translated_sentence_str = translate_sentence(model, src_sentence, max_src_len, max_tgt_len, device, tokenizer_hin)\\n                # Tokenize the predicted sentence for BLEU\\n                prediction = translated_sentence_str.split()\\n\\n                trgs.append(reference)\\n                pred_trgs.append(prediction)\\n\\n    # Calculate BLEU score\\n    # SmoothingFunction is used to handle cases where n-grams are not present in the reference\\n    smoothie = SmoothingFunction().method4\\n    bleu_score = sentence_bleu(trgs, pred_trgs, smoothing_function=smoothie)\\n\\n    return bleu_score\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "# Define model parameters\n",
        "INPUT_DIM = 100  # GloVe embedding dimension\n",
        "OUTPUT_DIM = hin_vocab_size  # IndicBERT vocabulary size\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2 # Set number of layers for both encoder and decoder\n",
        "\n",
        "# Instantiate the Attention mechanism\n",
        "attention = Attention(HIDDEN_DIM).to(device)\n",
        "'''\n",
        "\n",
        "# Instantiate the GRU-based Encoder and Decoder\n",
        "encoder = EncoderGRU(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
        "decoder = DecoderGRU(OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, attention).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with GRU components\n",
        "model = Seq2SeqGRU(encoder, decoder, device).to(device)\n",
        "'''\n",
        "\n",
        "\n",
        "# Instantiate the Bidirectional GRU-based Encoder\n",
        "encoder = EncoderBiGRU(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
        "# Instantiate the GRU-based Decoder (which expects a hidden_dim matching the encoder's output after projection)\n",
        "decoder = DecoderGRU(OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, attention).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with GRU components\n",
        "# Ensure the Seq2Seq model passes encoder_outputs and encoder_hidden correctly to the decoder\n",
        "model = Seq2SeqGRU(encoder, decoder, device).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# We ignore the padding token's contribution to the loss\n",
        "PAD_IDX = tokenizer_hin.pad_token_id\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(train_loader):\n",
        "        if i % 500 == 0:\n",
        "            print(f\"Batch {i}/{len(train_loader)}\")\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        # output: [batch_size, max_tgt_len, output_dim]\n",
        "        # trg: [batch_size, max_tgt_len]\n",
        "\n",
        "        # Reshape for CrossEntropyLoss\n",
        "        output = output[:, 1:].reshape(-1, output.shape[-1]) # Ignore <sos> token prediction\n",
        "        trg = trg[:, 1:].reshape(-1) # Ignore <sos> token ground truth\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {epoch_loss / len(train_loader):.3f}')\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(val_loader):\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output = model(src, trg)\n",
        "\n",
        "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Val Loss: {val_loss / len(val_loader):.3f}')\n",
        "\n",
        "\n",
        "'''\n",
        "def calculate_bleu(data_loader, model, max_src_len, max_tgt_len, device, tokenizer_hin):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(data_loader):\n",
        "            # We need the original target sentences as strings for BLEU, not the padded tensors\n",
        "            # The DataLoader provides the padded tensors, so we need to get the original\n",
        "            # sentences corresponding to this batch from the validation dataset.\n",
        "            # This is not ideal for large datasets, but for demonstration, we can\n",
        "            # retrieve the original sentences. A better approach would be to store\n",
        "            # original sentences in the dataset or pass them alongside tensors.\n",
        "\n",
        "            # For this example, we'll translate sentences one by one from the validation dataset\n",
        "            # based on the batch index, which is inefficient but works for demonstration.\n",
        "            # In a real scenario, process batches more efficiently.\n",
        "\n",
        "            batch_original_src = [source_sentences_val[j + i * data_loader.batch_size] for j in range(src.shape[0])]\n",
        "            batch_original_trg = [target_sentences_val[j + i * data_loader.batch_size] for j in range(trg.shape[0])]\n",
        "\n",
        "\n",
        "            for j in range(len(batch_original_src)):\n",
        "                src_sentence = batch_original_src[j]\n",
        "                trg_sentence = batch_original_trg[j]\n",
        "\n",
        "                # Tokenize reference sentence for BLEU\n",
        "                # IndicBERT tokenizer might produce sub-words, BLEU works best on word tokens\n",
        "                # Let's try tokenizing the reference using a simple split or a more robust tokenizer if needed\n",
        "                # For simplicity, we will use the IndicBERT tokenizer output for both prediction and reference\n",
        "                # This might slightly lower the BLEU score compared to using word tokens for reference.\n",
        "                # A more standard approach is to tokenize reference by spaces.\n",
        "                reference_tokens = tokenizer_hin(trg_sentence, truncation=False, padding=False)[\"input_ids\"]\n",
        "                # Remove special tokens from reference for BLEU calculation\n",
        "                reference_tokens_filtered = [tok for tok in reference_tokens if tok not in tokenizer_hin.all_special_ids]\n",
        "                reference = [tokenizer_hin.decode(reference_tokens_filtered).split()] # BLEU expects list of reference translations\n",
        "\n",
        "                # Translate the source sentence\n",
        "                translated_sentence_str = translate_sentence(model, src_sentence, max_src_len, max_tgt_len, device, tokenizer_hin)\n",
        "                # Tokenize the predicted sentence for BLEU\n",
        "                prediction = translated_sentence_str.split()\n",
        "\n",
        "                trgs.append(reference)\n",
        "                pred_trgs.append(prediction)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    # SmoothingFunction is used to handle cases where n-grams are not present in the reference\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_score = sentence_bleu(trgs, pred_trgs, smoothing_function=smoothie)\n",
        "\n",
        "    return bleu_score\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCSIbqqLuWIY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "b849f7b7-d802-4e32-e0de-111d52eb5b42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef translate_sentence(model, sentence, max_src_len, max_tgt_len, device, tokenizer_hin):\\n\\n    model.eval()\\n    with torch.no_grad():\\n        # Encode source sentence\\n        src_vec = encode_glove(sentence, max_src_len).unsqueeze(0).to(device) # Add batch dimension\\n        encoder_hidden = model.encoder(src_vec)\\n\\n        # Initialize decoder input with <sos> token\\n        # For IndicBERT, the CLS token (usually 101) serves as <sos>\\n        input_token = torch.tensor([tokenizer_hin.cls_token_id], device=device) # [1]\\n\\n        translated_tokens = []\\n\\n        for _ in range(max_tgt_len -1): # -1 because we already have the first token\\n            # Pass input token and hidden state to decoder\\n            output, encoder_hidden = model.decoder(input_token, encoder_hidden)\\n\\n            # Get the most probable next token\\n            pred_token = output.argmax(1) # [1]\\n\\n            # Add predicted token to list (excluding <sos> if it\\'s predicted later)\\n            if pred_token.item() != tokenizer_hin.cls_token_id:\\n                 translated_tokens.append(pred_token.item())\\n\\n            # Stop if <eos> token is predicted\\n            if pred_token.item() == tokenizer_hin.sep_token_id:\\n                break\\n\\n            # Use the predicted token as the next input\\n            input_token = pred_token\\n\\n        # Decode the translated tokens\\n        translated_sentence = tokenizer_hin.decode(translated_tokens, skip_special_tokens=True)\\n        \\n        while debugger < 10:\\n            print(f\"\\nTranslating sentence: \\'{sentence}\\'\")\\n            print(f\"Source tensor shape: {src_vec.shape}\")\\n            print(f\"Encoder final hidden state shape: {encoder_hidden.shape}\")\\n            print(f\"Translated sentence: \\'{translated_sentence}\\'\")\\n            debugger += 1\\n        \\n        return translated_sentence'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# debugger = 0\n",
        "# Evaluation with BLEU score\n",
        "'''\n",
        "def translate_sentence(model, sentence, max_src_len, max_tgt_len, device, tokenizer_hin):\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode source sentence\n",
        "        src_vec = encode_glove(sentence, max_src_len).unsqueeze(0).to(device) # Add batch dimension\n",
        "        encoder_hidden = model.encoder(src_vec)\n",
        "\n",
        "        # Initialize decoder input with <sos> token\n",
        "        # For IndicBERT, the CLS token (usually 101) serves as <sos>\n",
        "        input_token = torch.tensor([tokenizer_hin.cls_token_id], device=device) # [1]\n",
        "\n",
        "        translated_tokens = []\n",
        "\n",
        "        for _ in range(max_tgt_len -1): # -1 because we already have the first token\n",
        "            # Pass input token and hidden state to decoder\n",
        "            output, encoder_hidden = model.decoder(input_token, encoder_hidden)\n",
        "\n",
        "            # Get the most probable next token\n",
        "            pred_token = output.argmax(1) # [1]\n",
        "\n",
        "            # Add predicted token to list (excluding <sos> if it's predicted later)\n",
        "            if pred_token.item() != tokenizer_hin.cls_token_id:\n",
        "                 translated_tokens.append(pred_token.item())\n",
        "\n",
        "            # Stop if <eos> token is predicted\n",
        "            if pred_token.item() == tokenizer_hin.sep_token_id:\n",
        "                break\n",
        "\n",
        "            # Use the predicted token as the next input\n",
        "            input_token = pred_token\n",
        "\n",
        "        # Decode the translated tokens\n",
        "        translated_sentence = tokenizer_hin.decode(translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        while debugger < 10:\n",
        "            print(f\"\\nTranslating sentence: '{sentence}'\")\n",
        "            print(f\"Source tensor shape: {src_vec.shape}\")\n",
        "            print(f\"Encoder final hidden state shape: {encoder_hidden.shape}\")\n",
        "            print(f\"Translated sentence: '{translated_sentence}'\")\n",
        "            debugger += 1\n",
        "\n",
        "        return translated_sentence'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Saving Model Weights\n",
        "\n",
        "# Define a path to save the model\n",
        "MODEL_SAVE_PATH = 'seq2seq_gru_attention_bi_model.pth'\n",
        "\n",
        "print(f\"Saving model state to {MODEL_SAVE_PATH}...\")\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(\"Model state saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_Zi-D4L89uh",
        "outputId": "2e68ccbc-be6c-407e-9085-3fb989e7da24"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model state to seq2seq_gru_attention_bi_model.pth...\n",
            "Model state saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation with BLEU score (modified for attention)\n",
        "def translate_sentence(model, sentence, max_src_len, max_tgt_len, device, tokenizer_hin):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode source sentence\n",
        "        # The encoder returns (outputs, hidden) for nn.RNN\n",
        "        encoder_outputs, encoder_hidden = model.encoder(encode_glove(sentence, max_src_len).unsqueeze(0).to(device))\n",
        "\n",
        "        # The hidden state returned by the encoder is [num_layers, batch_size, hidden_dim]\n",
        "        # We use this directly as the initial hidden state for the decoder.\n",
        "        # If the decoder was an LSTM/GRU, we would need to handle the cell state too,\n",
        "        # but since it's also an RNN, the hidden state is sufficient.\n",
        "\n",
        "        input_token = torch.tensor([tokenizer_hin.cls_token_id], device=device)\n",
        "        translated_tokens = []\n",
        "\n",
        "        # Initialize decoder's hidden state directly from the encoder's final hidden state\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        for _ in range(max_tgt_len - 1):\n",
        "            # Decoder now expects hidden state (not a tuple) and encoder_outputs\n",
        "            output, decoder_hidden = model.decoder(input_token, decoder_hidden, encoder_outputs)\n",
        "            pred_token = output.argmax(1)\n",
        "\n",
        "            if pred_token.item() != tokenizer_hin.cls_token_id:\n",
        "                translated_tokens.append(pred_token.item())\n",
        "\n",
        "            if pred_token.item() == tokenizer_hin.sep_token_id:\n",
        "                break\n",
        "\n",
        "            input_token = pred_token\n",
        "\n",
        "        translated_sentence = tokenizer_hin.decode(translated_tokens, skip_special_tokens=True)\n",
        "        return translated_sentence"
      ],
      "metadata": {
        "id": "fmj9B03pmhyF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF5Sv6ibucI2",
        "outputId": "8e939eae-5e4b-4120-db13-f4a1cf492c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating BLEU score on the test set...\n",
            "Test BLEU score: 0.0092\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def calculate_bleu(data_loader, model, max_src_len, max_tgt_len, device, tokenizer_hin):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(data_loader):\n",
        "            # We need the original target sentences as strings for BLEU, not the padded tensors\n",
        "            # The DataLoader provides the padded tensors, so we need to get the original\n",
        "            # sentences corresponding to this batch from the validation dataset.\n",
        "            # This is not ideal for large datasets, but for demonstration, we can\n",
        "            # retrieve the original sentences. A better approach would be to store\n",
        "            # original sentences in the dataset or pass them alongside tensors.\n",
        "\n",
        "            # For this example, we'll translate sentences one by one from the validation dataset\n",
        "            # based on the batch index, which is inefficient but works for demonstration.\n",
        "            # In a real scenario, process batches more efficiently.\n",
        "\n",
        "            # When evaluating on the test set, you should retrieve original sentences\n",
        "            # from the test dataset lists, not the validation ones.\n",
        "            batch_original_src = [source_sentences_test[j + i * data_loader.batch_size] for j in range(src.shape[0])]\n",
        "            batch_original_trg = [target_sentences_test[j + i * data_loader.batch_size] for j in range(trg.shape[0])]\n",
        "\n",
        "\n",
        "            for j in range(len(batch_original_src)):\n",
        "                src_sentence = batch_original_src[j]\n",
        "                trg_sentence = batch_original_trg[j]\n",
        "\n",
        "                # Tokenize reference sentence for BLEU\n",
        "                # IndicBERT tokenizer might produce sub-words, BLEU works best on word tokens\n",
        "                # Let's try tokenizing the reference using a simple split or a more robust tokenizer if needed\n",
        "                # For simplicity, we will use the IndicBERT tokenizer output for both prediction and reference\n",
        "                # This might slightly lower the BLEU score compared to using word tokens for reference.\n",
        "                # A more standard approach is to tokenize reference by spaces.\n",
        "                reference_tokens = tokenizer_hin(trg_sentence, truncation=False, padding=False)[\"input_ids\"]\n",
        "                # Remove special tokens from reference for BLEU calculation\n",
        "                reference_tokens_filtered = [tok for tok in reference_tokens if tok not in tokenizer_hin.all_special_ids]\n",
        "                # Decode and split the filtered tokens into words\n",
        "                tokenized_reference = tokenizer_hin.decode(reference_tokens_filtered).split()\n",
        "\n",
        "                # Translate the source sentence\n",
        "                translated_sentence_str = translate_sentence(model, src_sentence, max_src_len, max_tgt_len, device, tokenizer_hin)\n",
        "                # Tokenize the predicted sentence for BLEU\n",
        "                prediction = translated_sentence_str.split()\n",
        "\n",
        "                # Append the reference as a list containing ONE list of tokens\n",
        "                trgs.append([tokenized_reference]) # BLEU expects list of reference translations for one candidate\n",
        "                # Append the prediction as a list of tokens\n",
        "                pred_trgs.append(prediction)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    # SmoothingFunction is used to handle cases where n-grams are not present in the reference\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    # Use corpus_bleu for calculating BLEU over multiple sentences\n",
        "    from nltk.translate.bleu_score import corpus_bleu\n",
        "    bleu_score = corpus_bleu(trgs, pred_trgs, smoothing_function=smoothie)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Load test data\n",
        "df_test = pd.read_csv(\"/content/translation_engtohin_test.csv\").dropna()\n",
        "source_sentences_test = df_test[\"source\"].tolist()\n",
        "target_sentences_test = df_test[\"target\"].tolist()\n",
        "\n",
        "test_dataset = TranslationDataset(source_sentences_test, target_sentences_test, max_src_len, max_tgt_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Calculate BLEU on the test set\n",
        "print(\"Calculating BLEU score on the test set...\")\n",
        "test_bleu_score = calculate_bleu(test_loader, model, max_src_len, max_tgt_len, device, tokenizer_hin)\n",
        "print(f\"Test BLEU score: {test_bleu_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd_CBp2Pcai1"
      },
      "source": [
        "Reading the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2rfBFvdW6LH",
        "outputId": "d0765551-a581-496f-bdcc-37eeb8b8ffb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              source  \\\n",
            "0  All these three centres, incidentally, are the...   \n",
            "1                 (For the full story, click here ).   \n",
            "2  He was also questioned whether action will be ...   \n",
            "3                    Students should not be worried.   \n",
            "4  Ridiculing BJP leaders, Yadav said when he was...   \n",
            "\n",
            "                                              target  \n",
            "0  दिलचस्प बात है कि ये तीनों केंद्र बीसीसीआई अध्...  \n",
            "1         (पूरा एपिसोड देखने के लिए यहां क्लिक करें)  \n",
            "2  उनसे भाजपा नेता कपिल मिश्रा के खिलाफ कार्रवाई ...  \n",
            "3                ' छात्रों को निराश नहीं किया जाएगा.  \n",
            "4  यादव ने कहा, 'रक्षा मंत्री रहते मैं खुद भारत-प...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Load the test dataset\n",
        "df = pd.read_csv('translation_engtohin_test.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcTa_nLEcl0T"
      },
      "source": [
        "Addition information:\n",
        "\n",
        "* Our suggestion is to use Google Colab’s free T4 GPU for training the RNN model. You can refer to this video: https://www.youtube.com/watch?v=GooQPDMH2Uk for guidance. To use the GPU, you should set the device as 'cuda' using:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').\n",
        "Then, make sure to move all model components and data tensors to this device using .to(device) before training and evaluation.\n",
        "* To save model weights use: \"torch.save(model.state_dict(), \"translation_model.pt\")\"\n",
        "* Pick any 5 sentences, print the source, target and predicted sentences for them.  \n",
        "* The code should be flexible such that the dimension of the input character embeddings, the hidden states of the encoders\n",
        " and decoders, and the number of layers in the encoder and decoder can be changed.\n",
        "* Mention the loss, optimizer used, learning rate and the number of epochs trained for.\n",
        "* Report the training loss, accuracy and validation loss,accuracy plots.\n",
        "* Use Bleu Score to evaluate your models performance\n",
        "* You are free to make any other changes in the code as you require"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft-RSsQKqliR"
      },
      "source": [
        "## Referencees\n",
        "\n",
        "https://nlp.stanford.edu/pubs/glove.pdf\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92d35cb2bb7448a786cdf685d8d3a12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f31a0ed1b4a648e4be42c06db3a0fb47",
              "IPY_MODEL_a21e2c5e61b24646966b2e26dfe0ba18",
              "IPY_MODEL_db5b215d02974e4f99a0301c16df8db7"
            ],
            "layout": "IPY_MODEL_53d03162461440b0b16e704fbb6cbaa3"
          }
        },
        "f31a0ed1b4a648e4be42c06db3a0fb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dbb93dfba2c4bc785e20628aaedf1c3",
            "placeholder": "​",
            "style": "IPY_MODEL_5e8a34cb80ed4762943a7f9062e4bd51",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a21e2c5e61b24646966b2e26dfe0ba18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc215e296b54ca9aafa19d4adb28906",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f6ddc28449b4ad1bf9730a6542b340d",
            "value": 51
          }
        },
        "db5b215d02974e4f99a0301c16df8db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b698854eeb9a48b58d74bae98e9c5f75",
            "placeholder": "​",
            "style": "IPY_MODEL_d25e51a8fd414049b07e166350dc0a63",
            "value": " 51.0/51.0 [00:00&lt;00:00, 4.96kB/s]"
          }
        },
        "53d03162461440b0b16e704fbb6cbaa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dbb93dfba2c4bc785e20628aaedf1c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8a34cb80ed4762943a7f9062e4bd51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fc215e296b54ca9aafa19d4adb28906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6ddc28449b4ad1bf9730a6542b340d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b698854eeb9a48b58d74bae98e9c5f75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d25e51a8fd414049b07e166350dc0a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8175b503fc484d99a96499625710eb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f5810f079f945c99f5b38fc8b185dba",
              "IPY_MODEL_99e8404b937e498ca430d9d437fd694d",
              "IPY_MODEL_7e8943f6f0ea48489b562d6cd9494c6c"
            ],
            "layout": "IPY_MODEL_8bfcbe39727f4ecda02af2c1c2a17d16"
          }
        },
        "2f5810f079f945c99f5b38fc8b185dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd8a28ad4184af9bf55e4a60783ec24",
            "placeholder": "​",
            "style": "IPY_MODEL_7f96be6bc4dc4c41b11fa8f2ed5a2b45",
            "value": "tokenizer.json: 100%"
          }
        },
        "99e8404b937e498ca430d9d437fd694d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e167afaa70fa45798627c9f59d836e88",
            "max": 7749482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3009b497865f4f178cb82f30e174ce1d",
            "value": 7749482
          }
        },
        "7e8943f6f0ea48489b562d6cd9494c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6512076f019f469d97981a68b06d6d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_c66c387c78fc43f9a179aa28d54469c9",
            "value": " 7.75M/7.75M [00:00&lt;00:00, 92.5MB/s]"
          }
        },
        "8bfcbe39727f4ecda02af2c1c2a17d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd8a28ad4184af9bf55e4a60783ec24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f96be6bc4dc4c41b11fa8f2ed5a2b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e167afaa70fa45798627c9f59d836e88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3009b497865f4f178cb82f30e174ce1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6512076f019f469d97981a68b06d6d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66c387c78fc43f9a179aa28d54469c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aace65d2af540b88a52bb9b51019b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1733c6a25adc439eb6696a7b9e4cf217",
              "IPY_MODEL_a70ff0cbefa34cd48cc9bb9c9c952f3a",
              "IPY_MODEL_a4d863767cff469ab58c0c141ea25019"
            ],
            "layout": "IPY_MODEL_d64b0c1703864d5eb1407e8a47e1852b"
          }
        },
        "1733c6a25adc439eb6696a7b9e4cf217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_703c314f4b9a4261b3db7357d91c6135",
            "placeholder": "​",
            "style": "IPY_MODEL_e2e078cf00cb49fc9f944ca6cb5753c3",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a70ff0cbefa34cd48cc9bb9c9c952f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fdb29d33c4405c80c0806cd1e9dc6c",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdecf511ce57433683520340e079695e",
            "value": 125
          }
        },
        "a4d863767cff469ab58c0c141ea25019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876ad96e521f47b5a967850042f30699",
            "placeholder": "​",
            "style": "IPY_MODEL_721ac8b489654569833b02f4ba7e053b",
            "value": " 125/125 [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "d64b0c1703864d5eb1407e8a47e1852b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "703c314f4b9a4261b3db7357d91c6135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e078cf00cb49fc9f944ca6cb5753c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5fdb29d33c4405c80c0806cd1e9dc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdecf511ce57433683520340e079695e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "876ad96e521f47b5a967850042f30699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721ac8b489654569833b02f4ba7e053b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}